{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /Users/aslonkhamidov/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "login(token=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-19 15:27:44.520 WARNING in 'deeppavlov.core.trainers.fit_trainer'['fit_trainer'] at line 66: TorchTrainer got additional init parameters ['pytest_max_batches', 'pytest_batch_size'] that will be ignored:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-19 15:27:45.41 INFO in 'deeppavlov.core.data.simple_vocab'['simple_vocab'] at line 104: [saving vocabulary to /Users/aslonkhamidov/.deeppavlov/models/ner_ontonotes_bert_torch_crf/tag.dict]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at DeepPavlov/bert-base-multilingual-cased-sentence and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2024-09-19 15:27:45.924 WARNING in 'deeppavlov.core.models.torch_model'['torch_model'] at line 96: Unable to place component TorchTransformersSequenceTagger on GPU, since no CUDA GPUs are available. Using CPU.\n",
      "2024-09-19 15:27:45.926 WARNING in 'deeppavlov.core.models.torch_model'['torch_model'] at line 151: Init from scratch. Load path /Users/aslonkhamidov/.deeppavlov/models/ner_ontonotes_bert_torch_crf/model.pth.tar does not exist.\n",
      "2024-09-19 15:27:45.928 WARNING in 'deeppavlov.models.torch_bert.torch_transformers_sequence_tagger'['torch_transformers_sequence_tagger'] at line 263: Init from scratch. Load path /Users/aslonkhamidov/.deeppavlov/models/ner_ontonotes_bert_torch_crf/model_crf.pth.tar does not exist.\n",
      "0it [00:00, ?it/s]/Users/aslonkhamidov/Desktop/code/consultant_ai/ner_for_names/env/lib/python3.11/site-packages/torchcrf/__init__.py:305: UserWarning: where received a uint8 condition tensor. This behavior is deprecated and will be removed in a future version of PyTorch. Use a boolean condition instead. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/TensorCompare.cpp:530.)\n",
      "  score = torch.where(mask[i].unsqueeze(1), next_score, score)\n",
      "1it [00:03,  3.34s/it]\n",
      "2024-09-19 15:27:49.279 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 198: Initial best ner_f1 of 0.1833\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"valid\": {\"eval_examples_count\": 44, \"metrics\": {\"ner_f1\": 0.1833, \"ner_token_f1\": 7.033}, \"time_spent\": \"0:00:04\", \"epochs_done\": 0, \"batches_seen\": 0, \"train_examples_seen\": 0, \"impatience\": 0, \"patience_limit\": 100}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:02,  2.78s/it]]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"train\": {\"eval_examples_count\": 60, \"metrics\": {\"ner_f1\": 0, \"ner_token_f1\": 0}, \"time_spent\": \"0:03:48\", \"epochs_done\": 0, \"batches_seen\": 20, \"train_examples_seen\": 1200, \"loss\": 0.471250731870532}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "1it [00:03,  3.30s/it]\n",
      "2024-09-19 15:31:36.450 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 211: Did not improve on the ner_f1 of 0.1833\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"valid\": {\"eval_examples_count\": 44, \"metrics\": {\"ner_f1\": 0, \"ner_token_f1\": 0}, \"time_spent\": \"0:03:51\", \"epochs_done\": 0, \"batches_seen\": 20, \"train_examples_seen\": 1200, \"impatience\": 1, \"patience_limit\": 100}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:04,  4.12s/it]]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"train\": {\"eval_examples_count\": 60, \"metrics\": {\"ner_f1\": 67.1233, \"ner_token_f1\": 75.2941}, \"time_spent\": \"0:06:48\", \"epochs_done\": 0, \"batches_seen\": 40, \"train_examples_seen\": 2400, \"loss\": 0.09816757114604116}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "1it [00:03,  3.31s/it]\n",
      "2024-09-19 15:34:36.509 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 206: Improved best ner_f1 from 0.1833 to 2.0305\n",
      "2024-09-19 15:34:36.509 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 208: Saving model\n",
      "2024-09-19 15:34:36.514 INFO in 'deeppavlov.core.models.torch_model'['torch_model'] at line 175: Saving model to /Users/aslonkhamidov/.deeppavlov/models/ner_ontonotes_bert_torch_crf/model.pth.tar.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"valid\": {\"eval_examples_count\": 44, \"metrics\": {\"ner_f1\": 2.0305, \"ner_token_f1\": 58.3333}, \"time_spent\": \"0:06:51\", \"epochs_done\": 0, \"batches_seen\": 40, \"train_examples_seen\": 2400, \"impatience\": 0, \"patience_limit\": 100}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:02,  2.47s/it]]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"train\": {\"eval_examples_count\": 60, \"metrics\": {\"ner_f1\": 89.4309, \"ner_token_f1\": 90.4762}, \"time_spent\": \"0:10:21\", \"epochs_done\": 0, \"batches_seen\": 60, \"train_examples_seen\": 3600, \"loss\": 0.03839089800603688}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "1it [00:03,  3.30s/it]\n",
      "2024-09-19 15:38:09.704 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 206: Improved best ner_f1 from 2.0305 to 3.2967\n",
      "2024-09-19 15:38:09.705 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 208: Saving model\n",
      "2024-09-19 15:38:09.706 INFO in 'deeppavlov.core.models.torch_model'['torch_model'] at line 175: Saving model to /Users/aslonkhamidov/.deeppavlov/models/ner_ontonotes_bert_torch_crf/model.pth.tar.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"valid\": {\"eval_examples_count\": 44, \"metrics\": {\"ner_f1\": 3.2967, \"ner_token_f1\": 68.0}, \"time_spent\": \"0:10:24\", \"epochs_done\": 0, \"batches_seen\": 60, \"train_examples_seen\": 3600, \"impatience\": 0, \"patience_limit\": 100}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "63it [11:14, 10.71s/it]\n",
      "1it [00:00,  1.61it/s]]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"train\": {\"eval_examples_count\": 60, \"metrics\": {\"ner_f1\": 91.9355, \"ner_token_f1\": 94.4882}, \"time_spent\": \"0:14:52\", \"epochs_done\": 1, \"batches_seen\": 80, \"train_examples_seen\": 4745, \"loss\": 0.029814852261915804}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "1it [00:04,  4.56s/it]\n",
      "2024-09-19 15:42:42.90 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 206: Improved best ner_f1 from 3.2967 to 6.5574\n",
      "2024-09-19 15:42:42.91 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 208: Saving model\n",
      "2024-09-19 15:42:42.92 INFO in 'deeppavlov.core.models.torch_model'['torch_model'] at line 175: Saving model to /Users/aslonkhamidov/.deeppavlov/models/ner_ontonotes_bert_torch_crf/model.pth.tar.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"valid\": {\"eval_examples_count\": 44, \"metrics\": {\"ner_f1\": 6.5574, \"ner_token_f1\": 61.0294}, \"time_spent\": \"0:14:57\", \"epochs_done\": 1, \"batches_seen\": 80, \"train_examples_seen\": 4745, \"impatience\": 0, \"patience_limit\": 100}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:05,  5.69s/it]]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"train\": {\"eval_examples_count\": 60, \"metrics\": {\"ner_f1\": 83.6066, \"ner_token_f1\": 85.0746}, \"time_spent\": \"0:18:14\", \"epochs_done\": 1, \"batches_seen\": 100, \"train_examples_seen\": 5945, \"loss\": 0.029786707181483508}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "1it [00:03,  3.22s/it]\n",
      "2024-09-19 15:46:03.85 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 206: Improved best ner_f1 from 6.5574 to 25.0\n",
      "2024-09-19 15:46:03.85 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 208: Saving model\n",
      "2024-09-19 15:46:03.89 INFO in 'deeppavlov.core.models.torch_model'['torch_model'] at line 175: Saving model to /Users/aslonkhamidov/.deeppavlov/models/ner_ontonotes_bert_torch_crf/model.pth.tar.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"valid\": {\"eval_examples_count\": 44, \"metrics\": {\"ner_f1\": 25.0, \"ner_token_f1\": 78.8644}, \"time_spent\": \"0:18:18\", \"epochs_done\": 1, \"batches_seen\": 100, \"train_examples_seen\": 5945, \"impatience\": 0, \"patience_limit\": 100}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  1.52it/s]]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"train\": {\"eval_examples_count\": 60, \"metrics\": {\"ner_f1\": 82.5397, \"ner_token_f1\": 90.6475}, \"time_spent\": \"0:21:21\", \"epochs_done\": 1, \"batches_seen\": 120, \"train_examples_seen\": 7145, \"loss\": 0.028148562414571643}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "1it [00:03,  3.27s/it]\n",
      "2024-09-19 15:49:10.132 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 206: Improved best ner_f1 from 25.0 to 26.8293\n",
      "2024-09-19 15:49:10.132 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 208: Saving model\n",
      "2024-09-19 15:49:10.134 INFO in 'deeppavlov.core.models.torch_model'['torch_model'] at line 175: Saving model to /Users/aslonkhamidov/.deeppavlov/models/ner_ontonotes_bert_torch_crf/model.pth.tar.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"valid\": {\"eval_examples_count\": 44, \"metrics\": {\"ner_f1\": 26.8293, \"ner_token_f1\": 86.1878}, \"time_spent\": \"0:21:25\", \"epochs_done\": 1, \"batches_seen\": 120, \"train_examples_seen\": 7145, \"impatience\": 0, \"patience_limit\": 100}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "63it [10:54, 10.38s/it]\n",
      "1it [00:00,  1.82it/s]]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"train\": {\"eval_examples_count\": 60, \"metrics\": {\"ner_f1\": 92.562, \"ner_token_f1\": 92.0635}, \"time_spent\": \"0:24:36\", \"epochs_done\": 2, \"batches_seen\": 140, \"train_examples_seen\": 8290, \"loss\": 0.022043067403137682}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "1it [00:03,  3.30s/it]\n",
      "2024-09-19 15:52:24.929 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 206: Improved best ner_f1 from 26.8293 to 40.0\n",
      "2024-09-19 15:52:24.929 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 208: Saving model\n",
      "2024-09-19 15:52:24.931 INFO in 'deeppavlov.core.models.torch_model'['torch_model'] at line 175: Saving model to /Users/aslonkhamidov/.deeppavlov/models/ner_ontonotes_bert_torch_crf/model.pth.tar.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"valid\": {\"eval_examples_count\": 44, \"metrics\": {\"ner_f1\": 40.0, \"ner_token_f1\": 89.1304}, \"time_spent\": \"0:24:39\", \"epochs_done\": 2, \"batches_seen\": 140, \"train_examples_seen\": 8290, \"impatience\": 0, \"patience_limit\": 100}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:02,  2.28s/it]]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"train\": {\"eval_examples_count\": 60, \"metrics\": {\"ner_f1\": 79.1045, \"ner_token_f1\": 89.3082}, \"time_spent\": \"0:28:31\", \"epochs_done\": 2, \"batches_seen\": 160, \"train_examples_seen\": 9490, \"loss\": 0.02206912781111896}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "1it [00:03,  3.14s/it]\n",
      "2024-09-19 15:56:19.191 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 206: Improved best ner_f1 from 40.0 to 42.3077\n",
      "2024-09-19 15:56:19.192 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 208: Saving model\n",
      "2024-09-19 15:56:19.193 INFO in 'deeppavlov.core.models.torch_model'['torch_model'] at line 175: Saving model to /Users/aslonkhamidov/.deeppavlov/models/ner_ontonotes_bert_torch_crf/model.pth.tar.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"valid\": {\"eval_examples_count\": 44, \"metrics\": {\"ner_f1\": 42.3077, \"ner_token_f1\": 90.2299}, \"time_spent\": \"0:28:34\", \"epochs_done\": 2, \"batches_seen\": 160, \"train_examples_seen\": 9490, \"impatience\": 0, \"patience_limit\": 100}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  1.16it/s]]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"train\": {\"eval_examples_count\": 60, \"metrics\": {\"ner_f1\": 92.8, \"ner_token_f1\": 95.6522}, \"time_spent\": \"0:31:14\", \"epochs_done\": 2, \"batches_seen\": 180, \"train_examples_seen\": 10690, \"loss\": 0.01976998713798821}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "1it [00:03,  3.18s/it]\n",
      "2024-09-19 15:59:02.446 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 206: Improved best ner_f1 from 42.3077 to 61.5385\n",
      "2024-09-19 15:59:02.446 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 208: Saving model\n",
      "2024-09-19 15:59:02.447 INFO in 'deeppavlov.core.models.torch_model'['torch_model'] at line 175: Saving model to /Users/aslonkhamidov/.deeppavlov/models/ner_ontonotes_bert_torch_crf/model.pth.tar.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"valid\": {\"eval_examples_count\": 44, \"metrics\": {\"ner_f1\": 61.5385, \"ner_token_f1\": 93.4066}, \"time_spent\": \"0:31:17\", \"epochs_done\": 2, \"batches_seen\": 180, \"train_examples_seen\": 10690, \"impatience\": 0, \"patience_limit\": 100}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "63it [11:05, 10.56s/it]\n",
      "1it [00:00,  1.58it/s]]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"train\": {\"eval_examples_count\": 60, \"metrics\": {\"ner_f1\": 89.6, \"ner_token_f1\": 92.0635}, \"time_spent\": \"0:35:17\", \"epochs_done\": 3, \"batches_seen\": 200, \"train_examples_seen\": 11835, \"loss\": 0.018095913669094445}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "1it [00:03,  3.48s/it]\n",
      "2024-09-19 16:03:05.927 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 211: Did not improve on the ner_f1 of 61.5385\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"valid\": {\"eval_examples_count\": 44, \"metrics\": {\"ner_f1\": 37.3333, \"ner_token_f1\": 88.5086}, \"time_spent\": \"0:35:20\", \"epochs_done\": 3, \"batches_seen\": 200, \"train_examples_seen\": 11835, \"impatience\": 1, \"patience_limit\": 100}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:02,  2.65s/it]]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"train\": {\"eval_examples_count\": 60, \"metrics\": {\"ner_f1\": 95.7983, \"ner_token_f1\": 94.4}, \"time_spent\": \"0:38:46\", \"epochs_done\": 3, \"batches_seen\": 220, \"train_examples_seen\": 13035, \"loss\": 0.016509905061684548}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "1it [00:03,  3.24s/it]\n",
      "2024-09-19 16:06:34.466 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 206: Improved best ner_f1 from 61.5385 to 70.8333\n",
      "2024-09-19 16:06:34.467 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 208: Saving model\n",
      "2024-09-19 16:06:34.469 INFO in 'deeppavlov.core.models.torch_model'['torch_model'] at line 175: Saving model to /Users/aslonkhamidov/.deeppavlov/models/ner_ontonotes_bert_torch_crf/model.pth.tar.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"valid\": {\"eval_examples_count\": 44, \"metrics\": {\"ner_f1\": 70.8333, \"ner_token_f1\": 94.0541}, \"time_spent\": \"0:38:49\", \"epochs_done\": 3, \"batches_seen\": 220, \"train_examples_seen\": 13035, \"impatience\": 0, \"patience_limit\": 100}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  1.46it/s]]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"train\": {\"eval_examples_count\": 60, \"metrics\": {\"ner_f1\": 93.2203, \"ner_token_f1\": 95.082}, \"time_spent\": \"0:42:12\", \"epochs_done\": 3, \"batches_seen\": 240, \"train_examples_seen\": 14235, \"loss\": 0.013154541002586484}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "1it [00:03,  3.29s/it]\n",
      "2024-09-19 16:10:01.8 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 206: Improved best ner_f1 from 70.8333 to 73.4694\n",
      "2024-09-19 16:10:01.9 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 208: Saving model\n",
      "2024-09-19 16:10:01.11 INFO in 'deeppavlov.core.models.torch_model'['torch_model'] at line 175: Saving model to /Users/aslonkhamidov/.deeppavlov/models/ner_ontonotes_bert_torch_crf/model.pth.tar.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"valid\": {\"eval_examples_count\": 44, \"metrics\": {\"ner_f1\": 73.4694, \"ner_token_f1\": 96.0212}, \"time_spent\": \"0:42:16\", \"epochs_done\": 3, \"batches_seen\": 240, \"train_examples_seen\": 14235, \"impatience\": 0, \"patience_limit\": 100}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "63it [10:25,  9.92s/it]\n",
      "1it [00:01,  1.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"train\": {\"eval_examples_count\": 60, \"metrics\": {\"ner_f1\": 97.561, \"ner_token_f1\": 97.8417}, \"time_spent\": \"0:45:18\", \"epochs_done\": 4, \"batches_seen\": 260, \"train_examples_seen\": 15380, \"loss\": 0.015467977826483547}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "1it [00:03,  3.24s/it]\n",
      "2024-09-19 16:13:06.551 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 206: Improved best ner_f1 from 73.4694 to 74.8299\n",
      "2024-09-19 16:13:06.551 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 208: Saving model\n",
      "2024-09-19 16:13:06.552 INFO in 'deeppavlov.core.models.torch_model'['torch_model'] at line 175: Saving model to /Users/aslonkhamidov/.deeppavlov/models/ner_ontonotes_bert_torch_crf/model.pth.tar.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"valid\": {\"eval_examples_count\": 44, \"metrics\": {\"ner_f1\": 74.8299, \"ner_token_f1\": 97.0976}, \"time_spent\": \"0:45:21\", \"epochs_done\": 4, \"batches_seen\": 260, \"train_examples_seen\": 15380, \"impatience\": 0, \"patience_limit\": 100}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:03,  3.69s/it]]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"train\": {\"eval_examples_count\": 60, \"metrics\": {\"ner_f1\": 96.0, \"ner_token_f1\": 98.6486}, \"time_spent\": \"0:49:02\", \"epochs_done\": 4, \"batches_seen\": 280, \"train_examples_seen\": 16580, \"loss\": 0.012834611034486442}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "1it [00:03,  3.21s/it]\n",
      "2024-09-19 16:16:50.362 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 206: Improved best ner_f1 from 74.8299 to 79.7297\n",
      "2024-09-19 16:16:50.362 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 208: Saving model\n",
      "2024-09-19 16:16:50.364 INFO in 'deeppavlov.core.models.torch_model'['torch_model'] at line 175: Saving model to /Users/aslonkhamidov/.deeppavlov/models/ner_ontonotes_bert_torch_crf/model.pth.tar.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"valid\": {\"eval_examples_count\": 44, \"metrics\": {\"ner_f1\": 79.7297, \"ner_token_f1\": 96.8254}, \"time_spent\": \"0:49:05\", \"epochs_done\": 4, \"batches_seen\": 280, \"train_examples_seen\": 16580, \"impatience\": 0, \"patience_limit\": 100}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:01,  1.57s/it]]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"train\": {\"eval_examples_count\": 60, \"metrics\": {\"ner_f1\": 95.0, \"ner_token_f1\": 95.3125}, \"time_spent\": \"0:51:51\", \"epochs_done\": 4, \"batches_seen\": 300, \"train_examples_seen\": 17780, \"loss\": 0.01247409472707659}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "1it [00:03,  3.23s/it]\n",
      "2024-09-19 16:19:39.609 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 211: Did not improve on the ner_f1 of 79.7297\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"valid\": {\"eval_examples_count\": 44, \"metrics\": {\"ner_f1\": 73.9726, \"ner_token_f1\": 95.3846}, \"time_spent\": \"0:51:54\", \"epochs_done\": 4, \"batches_seen\": 300, \"train_examples_seen\": 17780, \"impatience\": 1, \"patience_limit\": 100}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "63it [10:47, 10.28s/it]\n",
      "1it [00:00,  1.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"train\": {\"eval_examples_count\": 60, \"metrics\": {\"ner_f1\": 95.1613, \"ner_token_f1\": 96.8254}, \"time_spent\": \"0:54:50\", \"epochs_done\": 5, \"batches_seen\": 320, \"train_examples_seen\": 18925, \"loss\": 0.024715242523234336}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "1it [00:03,  3.26s/it]\n",
      "2024-09-19 16:22:38.597 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 211: Did not improve on the ner_f1 of 79.7297\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"valid\": {\"eval_examples_count\": 44, \"metrics\": {\"ner_f1\": 64.8276, \"ner_token_f1\": 94.2356}, \"time_spent\": \"0:54:53\", \"epochs_done\": 5, \"batches_seen\": 320, \"train_examples_seen\": 18925, \"impatience\": 2, \"patience_limit\": 100}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:01,  1.96s/it]]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"train\": {\"eval_examples_count\": 60, \"metrics\": {\"ner_f1\": 88.189, \"ner_token_f1\": 92.517}, \"time_spent\": \"0:57:53\", \"epochs_done\": 5, \"batches_seen\": 340, \"train_examples_seen\": 20125, \"loss\": 0.01440473342081532}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "1it [00:03,  3.21s/it]\n",
      "2024-09-19 16:25:41.988 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 211: Did not improve on the ner_f1 of 79.7297\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"valid\": {\"eval_examples_count\": 44, \"metrics\": {\"ner_f1\": 50.0, \"ner_token_f1\": 91.8519}, \"time_spent\": \"0:57:57\", \"epochs_done\": 5, \"batches_seen\": 340, \"train_examples_seen\": 20125, \"impatience\": 3, \"patience_limit\": 100}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:01,  1.03s/it]]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"train\": {\"eval_examples_count\": 60, \"metrics\": {\"ner_f1\": 97.4359, \"ner_token_f1\": 97.7778}, \"time_spent\": \"1:00:23\", \"epochs_done\": 5, \"batches_seen\": 360, \"train_examples_seen\": 21325, \"loss\": 0.015501191606745124}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "1it [00:03,  3.24s/it]\n",
      "2024-09-19 16:28:11.681 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 211: Did not improve on the ner_f1 of 79.7297\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"valid\": {\"eval_examples_count\": 44, \"metrics\": {\"ner_f1\": 77.2414, \"ner_token_f1\": 96.875}, \"time_spent\": \"1:00:26\", \"epochs_done\": 5, \"batches_seen\": 360, \"train_examples_seen\": 21325, \"impatience\": 4, \"patience_limit\": 100}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "63it [10:05,  9.61s/it]\n",
      "1it [00:01,  1.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"train\": {\"eval_examples_count\": 60, \"metrics\": {\"ner_f1\": 97.5207, \"ner_token_f1\": 98.5075}, \"time_spent\": \"1:04:50\", \"epochs_done\": 6, \"batches_seen\": 380, \"train_examples_seen\": 22470, \"loss\": 0.015331503347260878}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "1it [00:03,  3.57s/it]\n",
      "2024-09-19 16:32:38.761 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 211: Did not improve on the ner_f1 of 79.7297\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"valid\": {\"eval_examples_count\": 44, \"metrics\": {\"ner_f1\": 68.8742, \"ner_token_f1\": 95.6298}, \"time_spent\": \"1:04:53\", \"epochs_done\": 6, \"batches_seen\": 380, \"train_examples_seen\": 22470, \"impatience\": 5, \"patience_limit\": 100}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:01,  1.11s/it]]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"train\": {\"eval_examples_count\": 60, \"metrics\": {\"ner_f1\": 96.7213, \"ner_token_f1\": 97.1831}, \"time_spent\": \"1:08:37\", \"epochs_done\": 6, \"batches_seen\": 400, \"train_examples_seen\": 23670, \"loss\": 0.00895134630263783}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "1it [00:03,  3.24s/it]\n",
      "2024-09-19 16:36:26.154 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 211: Did not improve on the ner_f1 of 79.7297\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"valid\": {\"eval_examples_count\": 44, \"metrics\": {\"ner_f1\": 76.0, \"ner_token_f1\": 96.8912}, \"time_spent\": \"1:08:41\", \"epochs_done\": 6, \"batches_seen\": 400, \"train_examples_seen\": 23670, \"impatience\": 6, \"patience_limit\": 100}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  1.80it/s]]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"train\": {\"eval_examples_count\": 60, \"metrics\": {\"ner_f1\": 95.2381, \"ner_token_f1\": 96.2963}, \"time_spent\": \"1:11:13\", \"epochs_done\": 6, \"batches_seen\": 420, \"train_examples_seen\": 24870, \"loss\": 0.013796875323168933}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "1it [00:03,  3.48s/it]\n",
      "2024-09-19 16:39:02.368 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 206: Improved best ner_f1 from 79.7297 to 83.7838\n",
      "2024-09-19 16:39:02.369 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 208: Saving model\n",
      "2024-09-19 16:39:02.370 INFO in 'deeppavlov.core.models.torch_model'['torch_model'] at line 175: Saving model to /Users/aslonkhamidov/.deeppavlov/models/ner_ontonotes_bert_torch_crf/model.pth.tar.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"valid\": {\"eval_examples_count\": 44, \"metrics\": {\"ner_f1\": 83.7838, \"ner_token_f1\": 97.6623}, \"time_spent\": \"1:11:17\", \"epochs_done\": 6, \"batches_seen\": 420, \"train_examples_seen\": 24870, \"impatience\": 0, \"patience_limit\": 100}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  1.54it/s]]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"train\": {\"eval_examples_count\": 60, \"metrics\": {\"ner_f1\": 93.3333, \"ner_token_f1\": 92.6829}, \"time_spent\": \"1:15:16\", \"epochs_done\": 6, \"batches_seen\": 440, \"train_examples_seen\": 26070, \"loss\": 0.009633567434502766}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "1it [00:03,  3.14s/it]\n",
      "2024-09-19 16:43:05.13 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 206: Improved best ner_f1 from 83.7838 to 86.8966\n",
      "2024-09-19 16:43:05.13 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 208: Saving model\n",
      "2024-09-19 16:43:05.15 INFO in 'deeppavlov.core.models.torch_model'['torch_model'] at line 175: Saving model to /Users/aslonkhamidov/.deeppavlov/models/ner_ontonotes_bert_torch_crf/model.pth.tar.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"valid\": {\"eval_examples_count\": 44, \"metrics\": {\"ner_f1\": 86.8966, \"ner_token_f1\": 98.1627}, \"time_spent\": \"1:15:20\", \"epochs_done\": 6, \"batches_seen\": 440, \"train_examples_seen\": 26070, \"impatience\": 0, \"patience_limit\": 100}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "63it [10:46, 10.26s/it]\n",
      "1it [00:01,  1.18s/it]]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"train\": {\"eval_examples_count\": 60, \"metrics\": {\"ner_f1\": 94.3089, \"ner_token_f1\": 95.7746}, \"time_spent\": \"1:18:22\", \"epochs_done\": 7, \"batches_seen\": 460, \"train_examples_seen\": 27215, \"loss\": 0.01111619146540761}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "1it [00:03,  3.22s/it]\n",
      "2024-09-19 16:46:10.735 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 211: Did not improve on the ner_f1 of 86.8966\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"valid\": {\"eval_examples_count\": 44, \"metrics\": {\"ner_f1\": 76.5101, \"ner_token_f1\": 96.1637}, \"time_spent\": \"1:18:25\", \"epochs_done\": 7, \"batches_seen\": 460, \"train_examples_seen\": 27215, \"impatience\": 1, \"patience_limit\": 100}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "36it [06:26, 10.75s/it]\n",
      "2024-09-19 16:49:34.774 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 336: Stopped training\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at DeepPavlov/bert-base-multilingual-cased-sentence and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2024-09-19 16:49:36.577 WARNING in 'deeppavlov.core.models.torch_model'['torch_model'] at line 96: Unable to place component TorchTransformersSequenceTagger on GPU, since no CUDA GPUs are available. Using CPU.\n",
      "/Users/aslonkhamidov/Desktop/code/consultant_ai/ner_for_names/env/lib/python3.11/site-packages/deeppavlov/core/models/torch_model.py:134: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(weights_path, map_location=self.device)\n",
      "/Users/aslonkhamidov/Desktop/code/consultant_ai/ner_for_names/env/lib/python3.11/site-packages/deeppavlov/models/torch_bert/torch_transformers_sequence_tagger.py:260: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(weights_path_crf, map_location=self.device)\n",
      "1it [00:03,  3.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"valid\": {\"eval_examples_count\": 44, \"metrics\": {\"ner_f1\": 86.8966, \"ner_token_f1\": 98.1627}, \"time_spent\": \"0:00:04\"}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from deeppavlov import build_model, train_model\n",
    "\n",
    "# Load the configuration from the JSON file\n",
    "config_path = \"ner_onnotes_bert.json\"\n",
    "\n",
    "\n",
    "# Train the model\n",
    "def train_model_from_config(config_path):\n",
    "    # Load the configuration\n",
    "    # with open(config_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    #     config = json.load(f)\n",
    "\n",
    "    # Build and train the model\n",
    "    model = train_model(config_path)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model = train_model_from_config(config_path)\n",
    "    print(\"Model training complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aslonkhamidov/Desktop/code/consultant_ai/ner_for_names/env/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at DeepPavlov/bert-base-multilingual-cased-sentence and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2024-09-19 17:02:26.695 WARNING in 'deeppavlov.core.models.torch_model'['torch_model'] at line 96: Unable to place component TorchTransformersSequenceTagger on GPU, since no CUDA GPUs are available. Using CPU.\n",
      "/Users/aslonkhamidov/Desktop/code/consultant_ai/ner_for_names/env/lib/python3.11/site-packages/deeppavlov/core/models/torch_model.py:134: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(weights_path, map_location=self.device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[['Your', 'input', 'text', 'or', 'data', 'here']], [['O', 'O', 'O', 'O', 'O', 'O']]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aslonkhamidov/Desktop/code/consultant_ai/ner_for_names/env/lib/python3.11/site-packages/deeppavlov/models/torch_bert/torch_transformers_sequence_tagger.py:260: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(weights_path_crf, map_location=self.device)\n",
      "/Users/aslonkhamidov/Desktop/code/consultant_ai/ner_for_names/env/lib/python3.11/site-packages/torchcrf/__init__.py:305: UserWarning: where received a uint8 condition tensor. This behavior is deprecated and will be removed in a future version of PyTorch. Use a boolean condition instead. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/TensorCompare.cpp:530.)\n",
      "  score = torch.where(mask[i].unsqueeze(1), next_score, score)\n"
     ]
    }
   ],
   "source": [
    "# user the file\n",
    "from deeppavlov import build_model, configs\n",
    "\n",
    "# Path to your model configuration file\n",
    "# Replace 'my_model_config.json' with the correct path or configuration preset for your model\n",
    "config_path = 'ner_onnotes_bert.json'\n",
    "\n",
    "# Load the model\n",
    "model = build_model(config_path, download=False)\n",
    "print(\"Model build\")\n",
    "# Input for inference\n",
    "input_data = [\"Your input text or data here\"]\n",
    "\n",
    "# Run inference\n",
    "output = model(input_data)\n",
    "\n",
    "# Display the output\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\n",
    "    \"Abbos akaning bugun tu'gilgan kuni ekan.\",\n",
    "    \"O'zimni yaxshi ko'raman.\",\n",
    "    \"Men Nasibani yaxshi ko'raman.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['Abbos', 'akaning', 'bugun', \"tu'gilgan\", 'kuni', 'ekan', '.'],\n",
       "  [\"O'zimni\", 'yaxshi', \"ko'raman\", '.'],\n",
       "  ['Men', 'Nasibani', 'yaxshi', \"ko'raman\", '.']],\n",
       " [['B-PERSON', 'O', 'O', 'O', 'O', 'O', 'O'],\n",
       "  ['B-PERSON', 'O', 'O', 'O'],\n",
       "  ['O', 'B-PERSON', 'O', 'O', 'O']]]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Struct({'ner_ontonotes_bert': PosixPath('/Users/aslonkhamidov/Desktop/code/consultant_ai/ner_for_names/env/lib/python3.11/site-packages/deeppavlov/configs/ner/ner_ontonotes_bert.json'),\n",
       "        'ner_conll2003_deberta_crf': PosixPath('/Users/aslonkhamidov/Desktop/code/consultant_ai/ner_for_names/env/lib/python3.11/site-packages/deeppavlov/configs/ner/ner_conll2003_deberta_crf.json'),\n",
       "        'ner_rus_bert_probas': PosixPath('/Users/aslonkhamidov/Desktop/code/consultant_ai/ner_for_names/env/lib/python3.11/site-packages/deeppavlov/configs/ner/ner_rus_bert_probas.json'),\n",
       "        'ner_collection3_bert': PosixPath('/Users/aslonkhamidov/Desktop/code/consultant_ai/ner_for_names/env/lib/python3.11/site-packages/deeppavlov/configs/ner/ner_collection3_bert.json'),\n",
       "        'ner_bert_base': PosixPath('/Users/aslonkhamidov/Desktop/code/consultant_ai/ner_for_names/env/lib/python3.11/site-packages/deeppavlov/configs/ner/ner_bert_base.json'),\n",
       "        'ner_case_agnostic_mdistilbert': PosixPath('/Users/aslonkhamidov/Desktop/code/consultant_ai/ner_for_names/env/lib/python3.11/site-packages/deeppavlov/configs/ner/ner_case_agnostic_mdistilbert.json'),\n",
       "        'ner_rus_bert': PosixPath('/Users/aslonkhamidov/Desktop/code/consultant_ai/ner_for_names/env/lib/python3.11/site-packages/deeppavlov/configs/ner/ner_rus_bert.json'),\n",
       "        'ner_ontonotes_bert_mult': PosixPath('/Users/aslonkhamidov/Desktop/code/consultant_ai/ner_for_names/env/lib/python3.11/site-packages/deeppavlov/configs/ner/ner_ontonotes_bert_mult.json'),\n",
       "        'ner_ontonotes_deberta_crf': PosixPath('/Users/aslonkhamidov/Desktop/code/consultant_ai/ner_for_names/env/lib/python3.11/site-packages/deeppavlov/configs/ner/ner_ontonotes_deberta_crf.json'),\n",
       "        'ner_rus_convers_distilrubert_6L': PosixPath('/Users/aslonkhamidov/Desktop/code/consultant_ai/ner_for_names/env/lib/python3.11/site-packages/deeppavlov/configs/ner/ner_rus_convers_distilrubert_6L.json'),\n",
       "        'ner_conll2003_bert': PosixPath('/Users/aslonkhamidov/Desktop/code/consultant_ai/ner_for_names/env/lib/python3.11/site-packages/deeppavlov/configs/ner/ner_conll2003_bert.json'),\n",
       "        'ner_rus_convers_distilrubert_2L': PosixPath('/Users/aslonkhamidov/Desktop/code/consultant_ai/ner_for_names/env/lib/python3.11/site-packages/deeppavlov/configs/ner/ner_rus_convers_distilrubert_2L.json')})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "configs.ner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-19 17:19:38.478 INFO in 'deeppavlov.download'['download'] at line 138: Skipped http://files.deeppavlov.ai/v1/ner/ner_rus_bert_torch_new.tar.gz download because of matching hashes\n",
      "/Users/aslonkhamidov/Desktop/code/consultant_ai/ner_for_names/env/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at DeepPavlov/rubert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2024-09-19 17:19:39.474 WARNING in 'deeppavlov.core.models.torch_model'['torch_model'] at line 96: Unable to place component TorchTransformersSequenceTagger on GPU, since no CUDA GPUs are available. Using CPU.\n",
      "/Users/aslonkhamidov/Desktop/code/consultant_ai/ner_for_names/env/lib/python3.11/site-packages/deeppavlov/core/models/torch_model.py:134: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(weights_path, map_location=self.device)\n",
      "2024-09-19 17:19:41.829 ERROR in 'deeppavlov.core.common.params'['params'] at line 108: Exception in <class 'deeppavlov.models.torch_bert.torch_transformers_sequence_tagger.TorchTransformersSequenceTagger'>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/aslonkhamidov/Desktop/code/consultant_ai/ner_for_names/env/lib/python3.11/site-packages/deeppavlov/core/common/params.py\", line 102, in from_params\n",
      "    component = obj(**dict(config_params, **kwargs))\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/aslonkhamidov/Desktop/code/consultant_ai/ner_for_names/env/lib/python3.11/site-packages/deeppavlov/models/torch_bert/torch_transformers_sequence_tagger.py\", line 173, in __init__\n",
      "    super().__init__(model, **kwargs)\n",
      "  File \"/Users/aslonkhamidov/Desktop/code/consultant_ai/ner_for_names/env/lib/python3.11/site-packages/deeppavlov/core/models/torch_model.py\", line 84, in __init__\n",
      "    self.load()\n",
      "  File \"/Users/aslonkhamidov/Desktop/code/consultant_ai/ner_for_names/env/lib/python3.11/site-packages/deeppavlov/models/torch_bert/torch_transformers_sequence_tagger.py\", line 253, in load\n",
      "    super().load(fname)\n",
      "  File \"/Users/aslonkhamidov/Desktop/code/consultant_ai/ner_for_names/env/lib/python3.11/site-packages/deeppavlov/core/models/torch_model.py\", line 144, in load\n",
      "    self.model.load_state_dict(model_state)\n",
      "  File \"/Users/aslonkhamidov/Desktop/code/consultant_ai/ner_for_names/env/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 2215, in load_state_dict\n",
      "    raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n",
      "RuntimeError: Error(s) in loading state_dict for BertForTokenClassification:\n",
      "\tUnexpected key(s) in state_dict: \"bert.embeddings.position_ids\". \n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for BertForTokenClassification:\n\tUnexpected key(s) in state_dict: \"bert.embeddings.position_ids\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdeeppavlov\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m build_model, configs\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \n\u001b[0;32m----> 3\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfigs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mner_rus_bert\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownload\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# state_dict = torch.load('path_to_model.pth', map_location='cpu')\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# model.load_state_dict(state_dict, strict=False)\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/code/consultant_ai/ner_for_names/env/lib/python3.11/site-packages/deeppavlov/core/commands/infer.py:55\u001b[0m, in \u001b[0;36mbuild_model\u001b[0;34m(config, mode, load_trained, install, download)\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m         log\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNo \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msave_path\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m parameter for the \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m component, so \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mload_path\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m will not be renewed\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     53\u001b[0m                     \u001b[38;5;241m.\u001b[39mformat(component_config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclass_name\u001b[39m\u001b[38;5;124m'\u001b[39m, component_config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mref\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUNKNOWN\u001b[39m\u001b[38;5;124m'\u001b[39m))))\n\u001b[0;32m---> 55\u001b[0m component \u001b[38;5;241m=\u001b[39m \u001b[43mfrom_params\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcomponent_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m component_config:\n\u001b[1;32m     58\u001b[0m     model\u001b[38;5;241m.\u001b[39m_components_dict[component_config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m]] \u001b[38;5;241m=\u001b[39m component\n",
      "File \u001b[0;32m~/Desktop/code/consultant_ai/ner_for_names/env/lib/python3.11/site-packages/deeppavlov/core/common/params.py:102\u001b[0m, in \u001b[0;36mfrom_params\u001b[0;34m(params, mode, **kwargs)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmode\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m spec\u001b[38;5;241m.\u001b[39margs \u001b[38;5;241m+\u001b[39m spec\u001b[38;5;241m.\u001b[39mkwonlyargs \u001b[38;5;129;01mor\u001b[39;00m spec\u001b[38;5;241m.\u001b[39mvarkw \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    100\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmode\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m mode\n\u001b[0;32m--> 102\u001b[0m component \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconfig_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    104\u001b[0m     _refs[config_params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m]] \u001b[38;5;241m=\u001b[39m component\n",
      "File \u001b[0;32m~/Desktop/code/consultant_ai/ner_for_names/env/lib/python3.11/site-packages/deeppavlov/models/torch_bert/torch_transformers_sequence_tagger.py:173\u001b[0m, in \u001b[0;36mTorchTransformersSequenceTagger.__init__\u001b[0;34m(self, n_tags, pretrained_bert, bert_config_file, attention_probs_keep_prob, hidden_keep_prob, use_crf, **kwargs)\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ConfigError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo pre-trained BERT model is given.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcrf \u001b[38;5;241m=\u001b[39m CRF(n_tags) \u001b[38;5;28;01mif\u001b[39;00m use_crf \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/code/consultant_ai/ner_for_names/env/lib/python3.11/site-packages/deeppavlov/core/models/torch_model.py:84\u001b[0m, in \u001b[0;36mTorchModel.__init__\u001b[0;34m(self, model, device, optimizer, optimizer_parameters, learning_rate_drop_patience, learning_rate_drop_div, load_before_drop, min_learning_rate, clip_norm, *args, **kwargs)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_learning_rate \u001b[38;5;241m=\u001b[39m min_learning_rate\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclip_norm \u001b[38;5;241m=\u001b[39m clip_norm\n\u001b[0;32m---> 84\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;66;03m# we need to switch to eval mode here because by default it's in `train` mode.\u001b[39;00m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;66;03m# But in case of `interact/build_model` usage, we need to have model in eval mode.\u001b[39;00m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39meval()\n",
      "File \u001b[0;32m~/Desktop/code/consultant_ai/ner_for_names/env/lib/python3.11/site-packages/deeppavlov/models/torch_bert/torch_transformers_sequence_tagger.py:253\u001b[0m, in \u001b[0;36mTorchTransformersSequenceTagger.load\u001b[0;34m(self, fname)\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(\u001b[38;5;28mself\u001b[39m, fname\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 253\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    254\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcrf \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    255\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcrf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcrf\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n",
      "File \u001b[0;32m~/Desktop/code/consultant_ai/ner_for_names/env/lib/python3.11/site-packages/deeppavlov/core/models/torch_model.py:144\u001b[0m, in \u001b[0;36mTorchModel.load\u001b[0;34m(self, fname, *args, **kwargs)\u001b[0m\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mmodule\u001b[38;5;241m.\u001b[39mload_state_dict(model_state)\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 144\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:  \u001b[38;5;66;03m# TODO: remove this try-except after hf models deep update\u001b[39;00m\n\u001b[1;32m    146\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mload_state_dict(optimizer_state)\n",
      "File \u001b[0;32m~/Desktop/code/consultant_ai/ner_for_names/env/lib/python3.11/site-packages/torch/nn/modules/module.py:2215\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2210\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   2211\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2212\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[1;32m   2214\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2215\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2216\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   2217\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for BertForTokenClassification:\n\tUnexpected key(s) in state_dict: \"bert.embeddings.position_ids\". "
     ]
    }
   ],
   "source": [
    "from deeppavlov import build_model, configs\n",
    "import torch \n",
    "model = build_model(configs.ner.ner_rus_bert, download=True)\n",
    "# state_dict = torch.load('path_to_model.pth', map_location='cpu')\n",
    "# model.load_state_dict(state_dict, strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_model_uz = build_model(\"ner_ontonotes_bert.json\", download=True, load_trained=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
